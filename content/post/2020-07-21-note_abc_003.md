---
title: "Note: ABC, (3) 确定性递归模型"
date: 2020-07-21
math: true
authors:
    - noodler
categories:
    - 笔记
    - 读书
tags:
    - RBC
    - 读书笔记
---

> 现在我们的目标是要寻找政策函数（Policy Function）

## 1. 状态和控制

- 状态变量（State Variable）：时期$t$的状态变量指的是取值已经被确定了的量，其取值或者来自于过去的行为，或者来自于其他过程（自然）
- 控制变量（Control Variable）：时期$t$的控制变量是在该时期的个人为了最大化某目标函数值而明确选择的那些变量

## 2. 实例：简单RC模型

- 目标函数：$\sum_{i=o}^{\infty} \beta^i u(c_{t+i})$
- 单期预算约束：
  - $k_{t+1} = (1-\delta) k_t + i_t$
  - $y_t = f(k_t) \ge c_t + i_t$
- 整理：$c_t = f(k_t) - k_{t+1} + (1-\delta)k_t$
- 目标函数：$\sum_{t=o}^{\infty} \beta^t u(f(k_t) + (1-\delta)k_t - k_{t+1})$
- 状态变量：$k_t$
- 控制变量：$k_{t+1}$
- Bellman方程：$V(k) = \max_{\tilde{k}} \{u(f(k) + (1-\delta)k - \tilde{k}) + \beta V(\tilde{k})\}$
- FONC：$\beta V'(\tilde{k}) = u'(f(k) + (1-\delta)k - \tilde{k}) = u'(c)$
- Envelope条件：
  - $V'(k) = u'(c) [f'(k) + (1-\delta)]$
  - $V'(\tilde{k}) = u'(\tilde{c}) [f'(\tilde{k}) + (1-\delta)]$
- 代入一阶条件：$\frac{u'(c)}{u'(\tilde{c})} = \beta [f'(\tilde{k}) + (1-\delta)]$
- 稳态：$f'(\bar{k}) = \frac{1}{\beta}-1+\delta$

**值函数逼近**

- 生产函数：$f(k_t) = k_t^{\theta}$
- 效用函数：$u(c_t) = \ln (c_t)$
- Bellman方程：$V(k) = \max_{\tilde{k}} \{\ln (f(k) + (1-\delta)k - \tilde{k}) + \beta V(\tilde{k})\}$
- 参数：$\delta = 0.1, \theta = 0.36, \beta = 0.98$

**逼近过程**

- Step 1：
  - 计算稳态：$\theta \bar{k}^{\theta-1} = \frac{1}{\beta}-1+\delta, \bar{k} = 5.537$
- Step 2：
  - 初始化各变量
  - 状态变量：$k_1 = [0.1, 0.2, \cdots, 8.0]$
  - 控制变量：$k_2 = [0.1, 0.2, \cdots, 8.0]$
  - 政策函数：$PF = [0, 0, \cdots, 0]$
  - 初始值函数：$V_0 = [0, 0, \cdots, 0]$
  - 当期值函数：$V_1 = V_0$
  - 下一期值函数：$V_2 = V_0$
  - 状态$k_{1i}$下每个控制变量对应的目标值：$w_i = V_0$

- Step 3：值函数更新过程
---
For 第$i$个状态变量$k_{1i}$
- For 第$j$个控制变量$k_{2j}$
  - If $k_{1i}^{\theta} - k_{2j} + (1-\delta) k_{1ik} <= 0$
    - $w_i[j] = -10^8$
  - Else
    - $w_i[j] = \ln(k_{1i}^{\theta} - k_{2j} + (1-\delta) k_{1ik}) + \beta * V_2[j]$
- $V_1[i] = \max \{w_i\}$
- $PF[i] = k_2 [\arg \max_j {w_i[j]}]$
---

- Step 4：
  - 如果$V_1$和$V_2$差距比较大，则$V_2 = V_1$，返回 Step 3
  - 否则，结束


## 3. 实例：可变劳动的RC模型

- 生产函数：$y_t = f(k_t, h_t) = k_t^{\theta} h_t^{1-\theta}$
- 效用函数：$u(c_t, h_t) = \ln (c_t) + B \ln (1-h_t)$
- 预算约束：$k_{t+1} = (1-\delta) k_t + i_t$
- 生产可行性约束：$y_t = f(k_t, h_t) \ge c_t + i_t$
- 消费：$c_t = k_t^{\theta} h_t^{1-\theta} + (1-\delta) k_t - k_{t+1}$
- Bellman方程：$V(k) = \max_{\tilde{k}, h} \{u(f(k, h) + (1-\delta)k - \tilde{k}, h) + \beta V(\tilde{k})\}$
- 稳态条件：
  - $\theta k^{\theta-1}h^{1-\theta} = \frac{1}{\beta} - (1-\delta)$
  - $k^{\theta} h^{1-\theta} = c + \delta k$
  - $(1-\theta) k^{\theta} h^{-\theta} = \frac{B c}{1-h}$

**逼近过程**

- Step 1：
  - 计算稳态：根据稳态条件，三个方程三个未知数解出
- Step 2：
  - 初始化各变量
  - 状态变量：$k_1 = [0.1, 0.2, \cdots, 10.0]$
  - 控制变量：
    - $k_2 = [0.1, 0.2, \cdots, 8.0]$
    - $h = [0, 0.01, \cdots, 1]$
  - 政策函数：
    - $PFK = [0, 0, \cdots, 0]$
    - $PFH = [0, 0, \cdots, 0]$
  - 初始值函数：$V_0 = [0, 0, \cdots, 0]$
  - 当期值函数：$V_1 = V_0$
  - 下一期值函数：$V_2 = V_0$
  - 状态$k_{1i}$下每个控制变量对应的目标值：$w_i = V_0$

- Step 3：值函数更新过程
---
For 第$i$个状态变量$k_{1i}$
- For 第$j$个控制变量$k_{2j}$
  - For 第$p$个控制变量$h_p$
    - If $k_{1i}^{\theta}h_p^{1-\theta} - k_{2j} + (1-\delta) k_{1ik} <= 0$
      - $w_i[j, p] = -10^8$
    - Else
      - $w_i[j, p] = \ln(k_{1i}^{\theta}h_p^{1-\theta} - k_{2j} + (1-\delta) k_{1ik}) + B \ln (1-h_p) + \beta * V_2[j]$
- $V_1[i] = \max \{w_i\}$
- $j, p = \arg \max_{j, p} {w_i[j, p]}$
  - $PFK[i] = k_2[j]$
  - $PFH[i] = h[p]$
---

- Step 4：
  - 如果$V_1$和$V_2$差距比较大，则$V_2 = V_1$，返回 Step 3
  - 否则，结束
